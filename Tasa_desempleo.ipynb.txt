{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción de Tasa de Desempleo (Perú) con Red Neuronal Densa\n",
        "Fuente de datos: Banco Central de Reserva del Perú (BCRP API)\n"
      ],
      "metadata": {
        "id": "W8TQ140h4QnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Librerías\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n"
      ],
      "metadata": {
        "id": "2wxAJ3_11MWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bcrp_series(code: str) -> pd.DataFrame:\n",
        "    \"\"\"Descarga una serie del BCRP y devuelve un DataFrame con columnas ``date`` y ``value``.\n",
        "\n",
        "    Args:\n",
        "        code: Código de la serie publicado por el Banco Central de Reserva del Perú.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame con la fecha en formato ``datetime64[ns]`` y el valor numérico de la serie.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: Si la API devuelve un código de estado distinto de 200.\n",
        "    \"\"\"\n",
        "    url = f\"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/{code}/json/1960-01/2100-12/ing\"\n",
        "    response = requests.get(url, timeout=30)\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(f\"Error al obtener la serie {code}: {response.status_code}\")\n",
        "    payload = response.json()\n",
        "    entries = payload['periods']\n",
        "    df = pd.DataFrame(entries)\n",
        "    df['date'] = pd.to_datetime(df['name'], format='%b.%Y', errors='coerce')\n",
        "    df['value'] = pd.to_numeric(df['values'].str[0], errors='coerce')\n",
        "    return df[['date', 'value']]\n",
        "\n",
        "\n",
        "def download_series(code_map: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Descarga todas las series definidas y las devuelve renombradas por su alias.\n",
        "\n",
        "    Args:\n",
        "        code_map: Diccionario ``alias -> código`` con las series requeridas.\n",
        "\n",
        "    Returns:\n",
        "        Diccionario donde cada clave es el alias y cada valor el DataFrame asociado.\n",
        "    \"\"\"\n",
        "    datasets: Dict[str, pd.DataFrame] = {}\n",
        "    for alias, code in code_map.items():\n",
        "        series = get_bcrp_series(code)\n",
        "        datasets[alias] = series.rename(columns={'value': alias})\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def merge_series(datasets: Dict[str, pd.DataFrame], base_key: str = 'U') -> pd.DataFrame:\n",
        "    \"\"\"Une todas las series descargadas en un único DataFrame indexado por fecha.\n",
        "\n",
        "    Args:\n",
        "        datasets: Colección de series previamente descargadas.\n",
        "        base_key: Clave que identifica la serie principal utilizada como referencia temporal.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame con todas las series alineadas en el índice temporal.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: Si la clave ``base_key`` no está presente en ``datasets``.\n",
        "    \"\"\"\n",
        "    if base_key not in datasets:\n",
        "        raise KeyError(\"La serie base 'U' es obligatoria para construir el dataset\")\n",
        "    df = datasets[base_key].copy()\n",
        "    for alias, data in datasets.items():\n",
        "        if alias == base_key:\n",
        "            continue\n",
        "        df = pd.merge(df, data, on='date', how='outer')\n",
        "    df = df.sort_values('date').set_index('date')\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_lags(data: pd.DataFrame, column: str, lags: int) -> pd.DataFrame:\n",
        "    \"\"\"Agrega columnas con rezagos consecutivos a partir de una columna existente.\n",
        "\n",
        "    Args:\n",
        "        data: Conjunto de datos original.\n",
        "        column: Nombre de la columna sobre la que se calcularán los rezagos.\n",
        "        lags: Número de rezagos a generar.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame con columnas adicionales ``{column}_Lx`` donde ``x`` es el número de rezago.\n",
        "    \"\"\"\n",
        "    for lag in range(1, lags + 1):\n",
        "        data[f\"{column}_L{lag}\"] = data[column].shift(lag)\n",
        "    return data\n",
        "\n",
        "\n",
        "def prepare_dataframe(\n",
        "    datasets: Dict[str, pd.DataFrame],\n",
        "    lags: int = 12,\n",
        "    start_date: Optional[str] = '2007-01-01',\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Construye el DataFrame final listo para el modelado supervisado.\n",
        "\n",
        "    Args:\n",
        "        datasets: Diccionario de series descargadas desde la API del BCRP.\n",
        "        lags: Cantidad de rezagos a crear para las variables explicativas principales.\n",
        "        start_date: Fecha mínima del periodo a conservar en el análisis.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame limpio, interpolado y con rezagos, listo para alimentar el modelo.\n",
        "    \"\"\"\n",
        "    df = merge_series(datasets)\n",
        "    if 'pi' in df.columns:\n",
        "        df['pi'] = df['pi'].interpolate()\n",
        "        df['inflacion'] = df['pi'].pct_change(12) * 100\n",
        "    df = df.interpolate().dropna()\n",
        "    if start_date is not None:\n",
        "        df = df[df.index >= start_date]\n",
        "    for column in ['i', 'inflacion', 'E', 'Y', 'Xexp']:\n",
        "        if column in df.columns:\n",
        "            df = add_lags(df, column, lags)\n",
        "    df = df.dropna()\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "HCow_2glBDW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data(\n",
        "    df: pd.DataFrame,\n",
        "    target: str = 'U',\n",
        "    drop_columns: Optional[Tuple[str, ...]] = ('pi',),\n",
        "    test_size: float = 0.2,\n",
        "):\n",
        "    \"\"\"Genera los conjuntos de entrenamiento y prueba normalizados.\n",
        "\n",
        "    Args:\n",
        "        df: Conjunto de datos final procesado.\n",
        "        target: Nombre de la variable objetivo.\n",
        "        drop_columns: Columnas que deben excluirse del set de predictores.\n",
        "        test_size: Proporción de observaciones reservadas para prueba.\n",
        "\n",
        "    Returns:\n",
        "        Una tupla con (lista de características, X_train, X_test, y_train, y_test, scaler).\n",
        "    \"\"\"\n",
        "    excluded = {target}\n",
        "    if drop_columns:\n",
        "        excluded.update(drop_columns)\n",
        "    features = [col for col in df.columns if col not in excluded]\n",
        "    X = df[features].values\n",
        "    y = df[target].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=test_size, shuffle=False\n",
        "    )\n",
        "    return features, X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "\n",
        "def create_model(input_dim: int) -> Sequential:\n",
        "    \"\"\"Crea y compila la red neuronal densa utilizada para la predicción.\n",
        "\n",
        "    Args:\n",
        "        input_dim: Número de características de entrada del modelo.\n",
        "\n",
        "    Returns:\n",
        "        Instancia compilada de ``keras.Sequential`` lista para entrenamiento.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1),\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model: Sequential,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    patience: int = 20,\n",
        "    epochs: int = 500,\n",
        "    batch_size: int = 12,\n",
        "):\n",
        "    \"\"\"Entrena el modelo con early stopping y devuelve el historial de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        model: Red neuronal previamente compilada.\n",
        "        X_train: Matriz de entrenamiento normalizada.\n",
        "        y_train: Valores reales de entrenamiento.\n",
        "        X_val: Matriz de validación.\n",
        "        y_val: Valores reales de validación.\n",
        "        patience: Número de épocas sin mejora antes de detener el entrenamiento.\n",
        "        epochs: Número máximo de épocas a ejecutar.\n",
        "        batch_size: Tamaño del mini-lote utilizado en cada iteración.\n",
        "\n",
        "    Returns:\n",
        "        Objeto ``History`` de Keras con el registro de métricas por época.\n",
        "    \"\"\"\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1,\n",
        "    )\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model: Sequential, X_test, y_test) -> Tuple[float, float, List[float]]:\n",
        "    \"\"\"Evalúa el modelo y devuelve la pérdida, el MAE y las predicciones asociadas.\n",
        "\n",
        "    Args:\n",
        "        model: Red neuronal entrenada.\n",
        "        X_test: Matriz de características reservada para prueba.\n",
        "        y_test: Valores reales correspondientes a ``X_test``.\n",
        "\n",
        "    Returns:\n",
        "        Una tupla con (pérdida, MAE, lista de predicciones del modelo).\n",
        "    \"\"\"\n",
        "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "    predictions = model.predict(X_test).flatten().tolist()\n",
        "    return loss, mae, predictions\n",
        "\n",
        "\n",
        "def plot_predictions(dates: Sequence, y_true, y_pred) -> None:\n",
        "    \"\"\"Grafica las predicciones frente a los valores reales en una figura estándar.\n",
        "\n",
        "    Args:\n",
        "        dates: Índice temporal de los datos de prueba.\n",
        "        y_true: Valores reales observados.\n",
        "        y_pred: Predicciones generadas por el modelo.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(dates, y_true, label='Real', linewidth=2)\n",
        "    plt.plot(dates, y_pred, label='Predicho', linestyle='--')\n",
        "    plt.title('Predicción de la Tasa de Desempleo (Perú) con Red Neuronal Densa')\n",
        "    plt.xlabel('Fecha')\n",
        "    plt.ylabel('Tasa de Desempleo (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "vR4uY9r0CLiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(\n",
        "    code_map: Dict[str, str],\n",
        "    start_date: str = '2007-01-01',\n",
        "    test_size: float = 0.2,\n",
        ") -> Dict[str, object]:\n",
        "    \"\"\"Orquesta todos los pasos del flujo de trabajo y devuelve los artefactos clave.\n",
        "\n",
        "    Args:\n",
        "        code_map: Diccionario con los códigos de series a consultar en el BCRP.\n",
        "        start_date: Fecha inicial del periodo de análisis.\n",
        "        test_size: Proporción de datos reservados para la evaluación final.\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con los objetos intermedios (datasets, dataframe, modelo y métricas).\n",
        "    \"\"\"\n",
        "    datasets = download_series(code_map)\n",
        "    df = prepare_dataframe(datasets, lags=12, start_date=start_date)\n",
        "    features, X_train, X_test, y_train, y_test, scaler = prepare_training_data(\n",
        "        df,\n",
        "        target='U',\n",
        "        drop_columns=('pi',),\n",
        "        test_size=test_size,\n",
        "    )\n",
        "    model = create_model(X_train.shape[1])\n",
        "    history = train_model(model, X_train, y_train, X_test, y_test)\n",
        "    loss, mae, predictions = evaluate_model(model, X_test, y_test)\n",
        "    test_dates = df.index[-len(y_test):]\n",
        "    plot_predictions(test_dates, y_test, predictions)\n",
        "    return {\n",
        "        'datasets': datasets,\n",
        "        'dataframe': df,\n",
        "        'features': features,\n",
        "        'scaler': scaler,\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'metrics': {'loss': loss, 'mae': mae},\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test,\n",
        "        'test_dates': test_dates,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rcu_Y54cCyYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Ejecución del flujo completo para descargar datos, entrenar el modelo y mostrar resultados.\"\"\"\n",
        "codes = {\n",
        "    'U': 'PN38063GM',\n",
        "    'i': 'PD04722MM',\n",
        "    'pi': 'PN01271PM',\n",
        "    'E': 'PN01246PM',\n",
        "    'Y': 'PN38082AM',\n",
        "    'Xexp': 'PD38045AM',\n",
        "}\n",
        "\n",
        "results = run_pipeline(codes)\n",
        "print(f\"Error absoluto medio (MAE): {results['metrics']['mae']:.3f}\")\n",
        "print('\\nVariables predictoras utilizadas:')\n",
        "for feature in results['features']:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "comparison = pd.DataFrame(\n",
        "    {\n",
        "        'U_real': results['y_test'][:5],\n",
        "        'U_predicho': results['predictions'][:5],\n",
        "    },\n",
        "    index=results['test_dates'][:5],\n",
        ")\n",
        "comparison.index.name = 'Fecha'\n",
        "comparison\n"
      ],
      "metadata": {
        "id": "1iF72d6dF-v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}